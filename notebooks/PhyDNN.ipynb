{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from all_models import seed_everything,DNN\n",
    "import evaluation_and_visualization as ev_viz\n",
    "from all_models import MTNNConv_PressureVelocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Files\n"
     ]
    }
   ],
   "source": [
    "TRAINPERCENTAGE=0.55\n",
    "DATASETNAME=\"../../../ParticleDragForceEstimation/datasets/evaluation_data/AllData_with_Pressure_And_Velocity_Samples_case123/AllData_PRESSURE_and_VELOCITY_10_SAMPLES_All_RE_SF_Groups_TRAINPERCENTAGE_{}_With_Output_Mask_\".format(TRAINPERCENTAGE)\n",
    "exists = os.path.isfile(DATASETNAME+\"trainX.npy\")\n",
    "if exists:\n",
    "    X_train=np.load(DATASETNAME+\"trainX.npy\")\n",
    "    y_train=np.load(DATASETNAME+\"trainY.npy\")\n",
    "    X_val=np.load(DATASETNAME+\"valX.npy\")\n",
    "    y_val=np.load(DATASETNAME+\"valY.npy\")\n",
    "    X_test=np.load(DATASETNAME+\"testX.npy\")\n",
    "    y_test=np.load(DATASETNAME+\"testY.npy\")\n",
    "    print(\"Loaded Files\")\n",
    "else:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training and Testing Files\n",
      "(2639, 47) (2639, 29) (564, 47) (564, 29) (2621, 47) (2621, 29)\n"
     ]
    }
   ],
   "source": [
    "# y matrix structure\n",
    "#(Fx-nondim, PressureField1,...,PressureField10,VelocityXField1,...,VelocityXField10,Px,Py,Pz,TauX,TauY,Tauz,AvgFx-NonDim-Per_Re_SF,Mask)\n",
    "#Currently Mask is all ones.\n",
    "print(\"Shape of Training and Testing Files\")\n",
    "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes y_train = (2639, 29), y_val = (564, 29), y_test = (2621, 29)\n"
     ]
    }
   ],
   "source": [
    "#Mask specifically for pressure and velocity fields.\n",
    "mask_train=np.int32(y_train[:,-1])\n",
    "mask_val=np.int32(y_val[:,-1])\n",
    "mask_test=np.int32(y_test[:,-1])\n",
    "\n",
    "# normalize on training set and apply to test set\n",
    "std_scaler_x = StandardScaler()\n",
    "std_scaler_y = StandardScaler()\n",
    "std_scaler_pres=StandardScaler()\n",
    "std_scaler_vel=StandardScaler()\n",
    "std_scaler_presdragcomponents=StandardScaler()\n",
    "std_scaler_veldragcomponents=StandardScaler()\n",
    "\n",
    "# fit and transform on training set\n",
    "X_train = std_scaler_x.fit_transform(X_train)\n",
    "y_train[:, 0] = std_scaler_y.fit_transform(\n",
    "    np.reshape(y_train[:, 0], (y_train.shape[0], 1))\n",
    ")[:, 0]\n",
    "  \n",
    "#apply transformation on validation set.\n",
    "X_val = std_scaler_x.transform(X_val)\n",
    "y_val[:,0] = std_scaler_y.transform(\n",
    "     np.reshape(y_val[:,0], (y_val.shape[0],1))\n",
    "   )[:,0]\n",
    "\n",
    "# apply transformation on test set\n",
    "X_test = std_scaler_x.transform(X_test)\n",
    "y_test[:, 0] = std_scaler_y.transform(\n",
    "    np.reshape(y_test[:, 0], (y_test.shape[0], 1))\n",
    ")[:, 0]\n",
    "\n",
    "# # # #Standard Scaling of pressure y values. Only calculated for training.\n",
    "\n",
    "y_train[mask_train==1,1:11] = std_scaler_pres.fit_transform(y_train[mask_train==1,1:11])\n",
    "y_val[mask_val==1,1:11] = std_scaler_pres.transform(y_val[mask_val==1,1:11])\n",
    "y_test[mask_test==1,1:11] = std_scaler_pres.transform(y_test[mask_test==1,1:11])\n",
    "\n",
    "\n",
    "#### Standard Scaling of velocity y values.Only calculated for training.\n",
    "\n",
    "y_train[mask_train==1,11:21] = std_scaler_vel.fit_transform(y_train[mask_train==1,11:21])\n",
    "y_val[mask_val==1,11:21] = std_scaler_vel.transform(y_val[mask_val==1,11:21]) \n",
    "y_test[mask_test==1,11:21] = std_scaler_vel.transform(y_test[mask_test==1,11:21])\n",
    "\n",
    "##### Standard Scaling Of Pressure Drag Components.\n",
    "\n",
    "y_train[:,21:24] = std_scaler_presdragcomponents.fit_transform(y_train[:,21:24])\n",
    "y_val[:,21:24] = std_scaler_presdragcomponents.transform(y_val[:,21:24])\n",
    "y_test[:,21:24] = std_scaler_presdragcomponents.transform(y_test[:,21:24])\n",
    "\n",
    "\n",
    "###### Standard Scaling of Velocity Drag Components.\n",
    "\n",
    "y_train[:,24:27] = std_scaler_veldragcomponents.fit_transform(y_train[:,24:27])\n",
    "y_val[:,24:27] = std_scaler_veldragcomponents.transform(y_val[:,24:27])\n",
    "y_test[:,24:27] = std_scaler_veldragcomponents.transform(y_test[:,24:27])\n",
    "\n",
    "print(\"Shapes y_train = {}, y_val = {}, y_test = {}\".format(y_train.shape,y_val.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create A Separate Y_pdfmeans Numpy Array To Explicitly Constrain Distribution Means Of Pressure and Velocity Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2639, 2)\n"
     ]
    }
   ],
   "source": [
    "re_train=X_train[:,-2]\n",
    "sf_train=X_train[:,-1]\n",
    "tmppres=y_train[:,1:11]\n",
    "tmpvel=y_train[:,11:21]\n",
    "_tmp=np.hstack([re_train[:,None],sf_train[:,None],tmppres,tmpvel])\n",
    "columns=[\"Re\",\"Solidfraction\"]+[\"Pressure_X_{}\".format(i+1) for i in range(10)]+[\"Velocity_X_{}\".format(i+1) for i in range(10)]\n",
    "_tmpdftrain=pd.DataFrame(_tmp,columns=columns)\n",
    "\n",
    "\n",
    "prescols=[\"Pressure_X_{}\".format(i+1) for i in range(10)]\n",
    "velcols=[\"Velocity_X_{}\".format(i+1) for i in range(10)]\n",
    "pdf_means=dict()\n",
    "pdf_meansvel=dict()\n",
    "for key,val in _tmpdftrain.groupby(['Re','Solidfraction']):\n",
    "    pdf_means[key]=val[prescols].mean().mean()\n",
    "    pdf_meansvel[key]=val[velcols].mean().mean()\n",
    "    \n",
    "y_pdfmean=list()\n",
    "for row in _tmpdftrain.iterrows():\n",
    "    _re=row[1][0]\n",
    "    _sf=row[1][1]\n",
    "    y_pdfmean.append([pdf_means[(_re,_sf)],pdf_meansvel[(_re,_sf)]])\n",
    "y_pdfmean = np.array(y_pdfmean)\n",
    "print(y_pdfmean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# model settings\n",
    "D_in  = X_train.shape[1]\n",
    "output_size = 1    #1 FX-nondim value.\n",
    "output_size_aux1 = 10  #10 Pressure Sampled values.\n",
    "output_size_aux2 = 10  #30 Velocity sampled values.\n",
    "output_size_pdrag = 3\n",
    "output_size_vdrag = 3\n",
    "\n",
    "H = 128\n",
    "depth = 1\n",
    "depth_aux = 1\n",
    "shared_depth = 1\n",
    "NUMEPOCHS = 500\n",
    "Batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "#Best Parameters = 'presParam': 0.0001, 'velParam': 0.001, 'PDrag': 0.01, 'VDrag': 0.01, 'parampdfpresmean': 1.0, 'parampdfvelmean': 1.0\n",
    "# for parampres in [0.01,0.001,0.0001]:\n",
    "#     for paramvel in [0.1,0.01,0.001]:\n",
    "for parampres in [0.0001]:\n",
    "    for paramvel in [0.001]:\n",
    "        for paramdrag in [0.01]:\n",
    "            test_mses=list()\n",
    "            test_mres=list()\n",
    "            test_preds_all=list()\n",
    "            test_targets_all=list()\n",
    "            best_model=None\n",
    "            val_epochs=100\n",
    "            seed_everything(123)\n",
    "\n",
    "\n",
    "            param1=parampres\n",
    "            param2=paramvel\n",
    "            parampdrag=paramdrag\n",
    "            paramvdrag=paramdrag\n",
    "            parampdfpresmean=1.0\n",
    "            parampdfvelmean=1.0\n",
    "            \n",
    "            hyperparameters={'presParam':param1,'velParam':param2,'PDrag':parampdrag,'VDrag':paramvdrag,\n",
    "                      'parampdfpresmean':parampdfpresmean,'parampdfvelmean':parampdfvelmean}\n",
    "\n",
    "            \n",
    "            losses=list()\n",
    "            # Compile model   \n",
    "            model = MTNNConv_PressureVelocity(D_in, H, output_size,\n",
    "                        output_size_aux1,output_size_aux2,output_size_pdrag,output_size_vdrag, depth,depth_aux,\n",
    "                                              shared_depth,device=device,numpresdragcomponent=3,numsheardragcomponent=3).to(device)\n",
    "\n",
    "            # Loss Function\n",
    "            criterion = torch.nn.MSELoss() \n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "            # Input Data\n",
    "            trainX = Variable(torch.from_numpy(X_train).float()).to(device)\n",
    "            trainY = Variable(torch.from_numpy(y_train[:,0]).float()).to(device)\n",
    "            trainMask=torch.FloatTensor(torch.from_numpy(y_train[:,-1]).float()).to(device)\n",
    "            trainY_AUX1 = Variable(torch.from_numpy(y_train[:,1:11]).float()).to(device)\n",
    "            trainY_AUX2 = Variable(torch.from_numpy(y_train[:,11:21]).float()).to(device)\n",
    "            trainY_PDrag = Variable(torch.from_numpy(y_train[:,21:24]).float()).to(device)\n",
    "            trainY_VDrag = Variable(torch.from_numpy(y_train[:,24:27]).float()).to(device)\n",
    "            trainY_PDFPRESMEANS = Variable(torch.from_numpy(y_pdfmean[:,0]).float()).to(device)\n",
    "            trainY_PDFVELMEANS = Variable(torch.from_numpy(y_pdfmean[:,1]).float()).to(device) \n",
    "\n",
    "            valX = Variable(torch.from_numpy(X_val).float()).to(device)\n",
    "            valY = Variable(torch.from_numpy(y_val[:,0]).float()).to(device)\n",
    "            valMask=torch.FloatTensor(torch.from_numpy(y_val[:,-1]).float()).to(device)\n",
    "            valY_AUX1 = Variable(torch.from_numpy(y_val[:,1:11]).float()).to(device)\n",
    "            valY_AUX2 = Variable(torch.from_numpy(y_val[:,11:21]).float()).to(device)\n",
    "            valY_PDrag = Variable(torch.from_numpy(y_val[:,21:24]).float()).to(device)\n",
    "            valY_VDrag = Variable(torch.from_numpy(y_val[:,24:27]).float()).to(device)\n",
    "\n",
    "            testX = Variable(torch.from_numpy(X_test).float()).to(device)\n",
    "            testY = Variable(torch.from_numpy(y_test[:,0]).float()).to(device)\n",
    "            testMask=torch.FloatTensor(torch.from_numpy(y_test[:,-1]).float()).to(device)\n",
    "            testY_AUX1 = Variable(torch.from_numpy(y_test[:,1:11]).float()).to(device)\n",
    "            testY_AUX2 = Variable(torch.from_numpy(y_test[:,11:21]).float()).to(device)\n",
    "            testY_PDrag = Variable(torch.from_numpy(y_test[:,21:24]).float()).to(device)\n",
    "            testY_VDrag = Variable(torch.from_numpy(y_test[:,24:27]).float()).to(device)\n",
    "\n",
    "            # Train the model\n",
    "            data_train_loader = DataLoader(\n",
    "                list(zip(trainX,trainY,trainY_AUX1,trainY_AUX2,trainY_PDrag,trainY_VDrag,trainMask,trainY_PDFPRESMEANS,\n",
    "                         trainY_PDFVELMEANS)), \n",
    "                batch_size=Batch_size, \n",
    "                shuffle=True)\n",
    "            \n",
    "            data_val_loader = DataLoader(list(zip(valX,valY,valY_AUX1,valY_AUX2,valY_PDrag,valY_VDrag,valMask)),\n",
    "                                batch_size=Batch_size,shuffle=True)\n",
    "            \n",
    "            #Start Training\n",
    "            for epoch in range(NUMEPOCHS):\n",
    "                alltargets = list()\n",
    "                allpredictions = list()\n",
    "                allpredictions_aux_pres=list()\n",
    "                allpredictions_aux_vel=list()\n",
    "                allpredictions_pdrag=list()\n",
    "                allpredictions_vdrag=list()\n",
    "\n",
    "                epoch_losses_all=list()\n",
    "                epoch_losses_main=list()\n",
    "                epoch_losses_aux1=list()\n",
    "                epoch_losses_aux2=list()\n",
    "                epoch_losses_pdrag=list()\n",
    "                epoch_losses_vdrag=list()\n",
    "\n",
    "                for batchX, batchY,batchY_AUX_PRES,batchY_AUX_VEL,batchY_PDrag,batchY_VDrag,batchMask,batchY_PDFPRESMEAN,batchY_PDFVELMEAN in data_train_loader:\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs,outputs_AUX_PRES,outputs_AUX_VEL,outputs_Pdrag,outputs_Vdrag = model(batchX)\n",
    "\n",
    "                    outputs_AUX_PRES=outputs_AUX_PRES*batchMask.unsqueeze(1)\n",
    "                    outputs_AUX_VEL=outputs_AUX_VEL*batchMask.unsqueeze(1)\n",
    "                    \n",
    "                    loss1 = criterion(outputs.squeeze(), batchY)\n",
    "                    pres_loss = criterion(outputs_AUX_PRES,batchY_AUX_PRES)\n",
    "                    vel_loss = criterion(outputs_AUX_VEL,batchY_AUX_VEL)\n",
    "                    pdrag_loss = criterion(outputs_Pdrag,batchY_PDrag)\n",
    "                    vdrag_loss = criterion(outputs_Vdrag,batchY_VDrag)\n",
    "                    pdfpresmean_loss = criterion(outputs_AUX_PRES.mean(dim=1),batchY_PDFPRESMEAN.squeeze())\n",
    "                    pdfvelmean_loss = criterion(outputs_AUX_VEL.mean(dim=1),batchY_PDFVELMEAN.squeeze())\n",
    "\n",
    "                    loss = loss1 + param1*pres_loss + param2*vel_loss + parampdrag*pdrag_loss + \\\n",
    "                    paramvdrag*vdrag_loss + parampdfpresmean*pdfpresmean_loss + parampdfvelmean*pdfvelmean_loss\n",
    "\n",
    "                    allpredictions_aux_pres.append(outputs_AUX_PRES)\n",
    "                    allpredictions_aux_vel.append(outputs_AUX_VEL)\n",
    "                    allpredictions.append(outputs)\n",
    "                    alltargets.append(batchY)\n",
    "                    allpredictions_pdrag.append(outputs_Pdrag)\n",
    "                    allpredictions_vdrag.append(outputs_Vdrag)\n",
    "\n",
    "                    # Backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()     \n",
    "                    optimizer.step()\n",
    "                    losses.append(loss)\n",
    "                    epoch_losses_all.append(loss.item())\n",
    "                    epoch_losses_main.append(loss1.item())\n",
    "                    epoch_losses_aux1.append(pres_loss.item())\n",
    "                    epoch_losses_aux2.append(vel_loss.item())\n",
    "                    epoch_losses_pdrag.append(pdrag_loss.item())\n",
    "                    epoch_losses_vdrag.append(vdrag_loss.item())\n",
    "\n",
    "                if epoch%50==0:\n",
    "                    print(\"Epoch = {}\".format(epoch))\n",
    "\n",
    "            #Validate For Parameter Tuning\n",
    "            with torch.no_grad():\n",
    "                outputsVal,outputsVal_AUX_PRES,outputsVal_AUX_VEL,outputsVal_Pdrag,outputsVal_Vdrag = model(valX)\n",
    "                lossVal = criterion(outputsVal.squeeze(),valY)\n",
    "                print(\"Loss Val = {}\".format(lossVal.item()))\n",
    "                preds=std_scaler_y.inverse_transform(outputsVal.cpu().data.numpy().squeeze())\n",
    "                tgts=std_scaler_y.inverse_transform(valY.cpu().data.numpy().squeeze())\n",
    "\n",
    "                aurec = ev_viz.aurec(preds,tgts,y_val[:,-2])\n",
    "                \n",
    "                performance={'val_aurec':aurec}\n",
    "\n",
    "            #Update Settings Dict.\n",
    "            settings=dict()\n",
    "            settings['hyperparameters']=hyperparameters\n",
    "            settings['performance'] = performance\n",
    "            settings['model'] = model\n",
    "            \n",
    "            models.append(settings)\n",
    "            \n",
    "            print('\\nTraining Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Best models MSE,MRE,AUREC.\n",
    "best_model_aurec=None\n",
    "best_mse=-np.inf\n",
    "best_mre=-np.inf\n",
    "best_aurec=-np.inf\n",
    "for idx,m in enumerate(models):\n",
    "    hyp=m['hyperparameters']\n",
    "    perf=m['performance']\n",
    "    if perf['val_aurec']>best_aurec:\n",
    "        print(\"Best AUREC Idx = {}\".format(idx))\n",
    "        best_aurec=perf['val_aurec']\n",
    "        best_model_aurec=m['model']\n",
    "    \n",
    "\n",
    "    print(\"Hyper Parameters = {}\".format(hyp))\n",
    "    print(\"Performance = {}\".format(perf))\n",
    "    print(\"===========================================\\n\\n\")\n",
    "    \n",
    "\n",
    "#Test Best Model AUREC\n",
    "model=best_model_aurec\n",
    "# Test MSE (un_normalized) \n",
    "auxiliary_predictions_pressure=list()\n",
    "auxiliary_predictions_velocity=list()\n",
    "with torch.no_grad():\n",
    "    testpreds,testpreds_aux_pres,testpreds_aux_vel,outputsPDrag,outputsVDrag = model(testX)\n",
    "    preds = torch.from_numpy(\n",
    "        std_scaler_y.inverse_transform(testpreds.cpu().detach().squeeze())\n",
    "    ).float()\n",
    "\n",
    "    re=std_scaler_x.inverse_transform(testX.to(\"cpu\").data.numpy())[:,-2]\n",
    "    sf=std_scaler_x.inverse_transform(testX.to(\"cpu\").data.numpy())[:,-1]\n",
    "\n",
    "    tgts = torch.from_numpy(\n",
    "        std_scaler_y.inverse_transform(testY.cpu().data.numpy().squeeze())\n",
    "    ).float()\n",
    "\n",
    "    auxiliary_predictions_pressure = std_scaler_pres.inverse_transform(testpreds_aux_pres.to(\"cpu\").data.numpy())\n",
    "    auxiliary_predictions_velocity = std_scaler_vel.inverse_transform(testpreds_aux_vel.to(\"cpu\").data.numpy())\n",
    "\n",
    "    aurec=ev_viz.aurec(preds.squeeze().data.cpu().numpy(),tgts.squeeze().data.cpu().numpy(),y_test[:,-2])\n",
    "    print(\"AUREC = {}\".format(aurec))\n",
    "    print(\"===============================================================\\n\")\n",
    "\n",
    "\n",
    "#SAVE Model and Predictions AUREC\n",
    "MODEL='PhyDNNAll-PDFPRESMEAN-{}-PDFVELMEAN-{}-PX-TauX'.format(parampdfpresmean,parampdfvelmean)\n",
    "_tmp=std_scaler_x.inverse_transform(X_test)\n",
    "preds_pres=std_scaler_pres.inverse_transform(testpreds_aux_pres.cpu().data.numpy())\n",
    "preds_vel=std_scaler_vel.inverse_transform(testpreds_aux_vel.cpu().data.numpy())\n",
    "act_pres=std_scaler_pres.inverse_transform(testY_AUX1.cpu().data.numpy())\n",
    "act_vel=std_scaler_vel.inverse_transform(testY_AUX2.cpu().data.numpy())\n",
    "preds_pdrag=std_scaler_presdragcomponents.inverse_transform(outputsPDrag.cpu().data.numpy())\n",
    "preds_vdrag=std_scaler_veldragcomponents.inverse_transform(outputsVDrag.cpu().data.numpy())\n",
    "act_pdrag=std_scaler_presdragcomponents.inverse_transform(testY_PDrag.cpu().data.numpy())\n",
    "act_vdrag=std_scaler_veldragcomponents.inverse_transform(testY_VDrag.cpu().data.numpy())\n",
    "\n",
    "\n",
    "#Structure of output predictions dataframe: \n",
    "\"\"\"\n",
    "[  \n",
    "   Fx-Nondim_Prediction,\n",
    "   Fx-Nondim_Target,\n",
    "   Mean_Per_Re_Sf,\n",
    "   Re,\n",
    "   Sf,\n",
    "   pressurepred1,\n",
    "   ...\n",
    "   pressurepred10,\n",
    "   velpred1,\n",
    "   ...\n",
    "   velpred10,\n",
    "   pressureactual1,\n",
    "   ...\n",
    "   pressureactual10,\n",
    "   velactual1,\n",
    "   ...\n",
    "   velactual10,\n",
    "   Px_pred,\n",
    "   Py_pred,\n",
    "   Pz_pred\n",
    "   TauX_pred,\n",
    "   TauY_pred,\n",
    "   TauZ_pred,\n",
    "   Px_Actual,\n",
    "   Py_Actual,\n",
    "   Pz_Actual,\n",
    "   TauX_Actual,\n",
    "   TauY_Actual,\n",
    "   TauZ_Actual\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "_tmp2=np.hstack([preds.to(\"cpu\").data.numpy().ravel()[:,None],\n",
    "                 tgts.to(\"cpu\").data.numpy().ravel()[:,None],\n",
    "                 y_test[:,-2].squeeze()[:,None],\n",
    "                 _tmp[:,-2].squeeze()[:,None],\n",
    "                 _tmp[:,-1].squeeze()[:,None],\n",
    "                 preds_pres,preds_vel,act_pres,act_vel,preds_pdrag,preds_vdrag,act_pdrag,act_vdrag])\n",
    "\n",
    "pres_cols=['PressurePredictions_{}'.format(i+1) for i in range(10)]\n",
    "vel_cols=['VelocityPredictions_{}'.format(i+1) for i in range(10)]\n",
    "pres_act_cols=['PressureActual_{}'.format(i+1) for i in range(10)]\n",
    "vel_act_cols=['VelocityActual_{}'.format(i+1) for i in range(10)]\n",
    "pdrag_pred_cols=['Px_Predictions','Py_Predictions','Pz_Predictions']\n",
    "vdrag_pred_cols=['Taux_Predictions','Tauy_Predictions','Tauz_Predictions']\n",
    "pdrag_act_cols=['Px','Py','Pz']\n",
    "vdrag_act_cols=['Taux','Tauy','Tauz']\n",
    "\n",
    "_df=pd.DataFrame(_tmp2,columns=['{}-Predictions'.format(MODEL),'Targets','Mean','Re','Solidfraction']+\\\n",
    "                 pres_cols+vel_cols+pres_act_cols+vel_act_cols+pdrag_pred_cols+vdrag_pred_cols+\\\n",
    "                 pdrag_act_cols+vdrag_act_cols)\n",
    "\n",
    "\n",
    "outputfile=\"/home/nik90/experiments/particleDragForce/dnn_mt_seq/{}_case1_predictions_TRAINPERCENTAGE_{}.csv\".format(MODEL,TRAINPERCENTAGE)\n",
    "_df.to_csv(outputfile,index=False)\n",
    "#Save Model\n",
    "#torch.save(model.state_dict(),\"../../../../models/Nikhil_{}_Case1_TrainPercentage_{}\".format(MODEL,TRAINPERCENTAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
